--- ../src/slurmctld/job_scheduler.c	2011-09-12 17:07:40.000000000 +0300
@@ -424,211 +424,6 @@
 
 	debug("sched: Running job scheduler");
 	job_queue = build_job_queue(false);
-	while ((job_queue_rec = list_pop_bottom(job_queue, sort_job_queue2))) {
-		job_ptr  = job_queue_rec->job_ptr;
-		part_ptr = job_queue_rec->part_ptr;
-		xfree(job_queue_rec);
-		if ((time(NULL) - sched_start) >= sched_timeout) {
-			debug("sched: loop taking too long, breaking out");
-			break;
-		}
-		if (job_depth++ > job_limit) {
-			debug3("sched: already tested %u jobs, breaking out",
-			       job_depth);
-			break;
-		}
-		if (!IS_JOB_PENDING(job_ptr))
-			continue;	/* started in other partition */
-		if (job_ptr->priority == 0)	{ /* held */
-			debug3("sched: JobId=%u. State=%s. Reason=%s. "
-			       "Priority=%u.",
-			       job_ptr->job_id,
-			       job_state_string(job_ptr->job_state),
-			       job_reason_string(job_ptr->state_reason),
-			       job_ptr->priority);
-			continue;
-		}
-
-		/* If a patition update has occurred, then do a limit check. */
-		if (save_last_part_update != last_part_update) {
-			int fail_reason = job_limits_check(&job_ptr);
-			if (fail_reason != WAIT_NO_REASON) {
-				job_ptr->state_reason = fail_reason;
-				job_ptr->priority = 1;
-				continue;
-			}
-		} else if ((job_ptr->state_reason == WAIT_PART_TIME_LIMIT) ||
-			   (job_ptr->state_reason == WAIT_PART_NODE_LIMIT)) {
-				job_ptr->start_time = 0;
-				job_ptr->priority = 1;
-				continue;
-		}
-		if (job_ptr->part_ptr != part_ptr) {
-			/* Cycle through partitions usable for this job */
-			job_ptr->part_ptr = part_ptr;
-		}
-		if ((job_ptr->resv_name == NULL) &&
-		    _failed_partition(job_ptr->part_ptr, failed_parts,
-				      failed_part_cnt)) {
-			if (job_ptr->priority != 1) {	/* not system hold */
-				job_ptr->state_reason = WAIT_PRIORITY;
-				xfree(job_ptr->state_desc);
-			}
-			debug3("sched: JobId=%u. State=%s. Reason=%s. "
-			       "Priority=%u. Partition=%s.",
-			       job_ptr->job_id,
-			       job_state_string(job_ptr->job_state),
-			       job_reason_string(job_ptr->state_reason),
-			       job_ptr->priority,
-			       job_ptr->partition);
-			continue;
-		}
-		if (bit_overlap(avail_node_bitmap,
-				job_ptr->part_ptr->node_bitmap) == 0) {
-			/* All nodes DRAIN, DOWN, or
-			 * reserved for jobs in higher priority partition */
-			job_ptr->state_reason = WAIT_RESOURCES;
-			debug3("sched: JobId=%u. State=%s. Reason=%s. "
-			       "Priority=%u. Partition=%s.",
-			       job_ptr->job_id,
-			       job_state_string(job_ptr->job_state),
-			       job_reason_string(job_ptr->state_reason),
-			       job_ptr->priority,
-			       job_ptr->partition);
-			continue;
-		}
-		if (license_job_test(job_ptr, time(NULL)) != SLURM_SUCCESS) {
-			job_ptr->state_reason = WAIT_LICENSES;
-			xfree(job_ptr->state_desc);
-			debug3("sched: JobId=%u. State=%s. Reason=%s. "
-			       "Priority=%u.",
-			       job_ptr->job_id,
-			       job_state_string(job_ptr->job_state),
-			       job_reason_string(job_ptr->state_reason),
-			       job_ptr->priority);
-			continue;
-		}
-
-		if (assoc_mgr_validate_assoc_id(acct_db_conn,
-						job_ptr->assoc_id,
-						accounting_enforce)) {
-			/* NOTE: This only happens if a user's account is
-			 * disabled between when the job was submitted and
-			 * the time we consider running it. It should be
-			 * very rare. */
-			info("sched: JobId=%u has invalid account",
-			     job_ptr->job_id);
-			last_job_update = time(NULL);
-			job_ptr->job_state = JOB_FAILED;
-			job_ptr->exit_code = 1;
-			job_ptr->state_reason = FAIL_ACCOUNT;
-			xfree(job_ptr->state_desc);
-			job_ptr->start_time = job_ptr->end_time = time(NULL);
-			job_completion_logger(job_ptr, false);
-			delete_job_details(job_ptr);
-			continue;
-		}
-
-		error_code = select_nodes(job_ptr, false, NULL);
-		if (error_code == ESLURM_NODES_BUSY) {
-			debug3("sched: JobId=%u. State=%s. Reason=%s. "
-			       "Priority=%u. Partition=%s.",
-			       job_ptr->job_id,
-			       job_state_string(job_ptr->job_state),
-			       job_reason_string(job_ptr->state_reason),
-			       job_ptr->priority, job_ptr->partition);
-			bool fail_by_part = true;
-#ifdef HAVE_BG
-			/* When we use static or overlap partitioning on
-			 * BlueGene, each job can possibly be scheduled
-			 * independently, without impacting other jobs of
-			 * different sizes. Therefore we sort and try to
-			 * schedule every pending job unless the backfill
-			 * scheduler is configured. */
-			if (!backfill_sched)
-				fail_by_part = false;
-#endif
-			if (fail_by_part) {
-		 		/* do not schedule more jobs in this partition
-				 * or on nodes in this partition */
-				failed_parts[failed_part_cnt++] =
-						job_ptr->part_ptr;
-				bit_not(job_ptr->part_ptr->node_bitmap);
-				bit_and(avail_node_bitmap,
-					job_ptr->part_ptr->node_bitmap);
-				bit_not(job_ptr->part_ptr->node_bitmap);
-			}
-		} else if (error_code == ESLURM_RESERVATION_NOT_USABLE) {
-			if (job_ptr->resv_ptr &&
-			    job_ptr->resv_ptr->node_bitmap) {
-				debug3("sched: JobId=%u. State=%s. "
-				       "Reason=%s. Priority=%u.",
-				       job_ptr->job_id,
-				       job_state_string(job_ptr->job_state),
-				       job_reason_string(job_ptr->
-							 state_reason),
-				       job_ptr->priority);
-				bit_not(job_ptr->resv_ptr->node_bitmap);
-				bit_and(avail_node_bitmap,
-					job_ptr->resv_ptr->node_bitmap);
-				bit_not(job_ptr->resv_ptr->node_bitmap);
-			} else {
-				/* The job has no reservation but requires
-				 * nodes that are currently in some reservation
-				 * so just skip over this job and try running
-				 * the next lower priority job */
-				debug3("sched: JobId=%u State=%s. "
-				       "Reason=Required nodes are reserved."
-				       "Priority=%u",job_ptr->job_id,
-				       job_state_string(job_ptr->job_state),
-				       job_ptr->priority);
-			}
-		} else if (error_code == SLURM_SUCCESS) {
-			/* job initiated */
-			debug3("sched: JobId=%u initiated", job_ptr->job_id);
-			last_job_update = now;
-#ifdef HAVE_BG
-			select_g_select_jobinfo_get(job_ptr->select_jobinfo,
-						    SELECT_JOBDATA_IONODES,
-						    &ionodes);
-			if(ionodes) {
-				sprintf(tmp_char,"%s[%s]",
-					job_ptr->nodes, ionodes);
-			} else {
-				sprintf(tmp_char,"%s",job_ptr->nodes);
-			}
-			info("sched: Allocate JobId=%u BPList=%s",
-			     job_ptr->job_id, tmp_char);
-			xfree(ionodes);
-#else
-			info("sched: Allocate JobId=%u NodeList=%s #CPUs=%u",
-			     job_ptr->job_id, job_ptr->nodes,
-			     job_ptr->total_cpus);
-#endif
-			if (job_ptr->batch_flag == 0)
-				srun_allocate(job_ptr->job_id);
-			else if (job_ptr->details->prolog_running == 0)
-				launch_job(job_ptr);
-			rebuild_job_part_list(job_ptr);
-			job_cnt++;
-		} else if ((error_code !=
-			    ESLURM_REQUESTED_PART_CONFIG_UNAVAILABLE) &&
-			   (error_code != ESLURM_NODE_NOT_AVAIL)      &&
-			   (error_code != ESLURM_ACCOUNTING_POLICY)) {
-			info("sched: schedule: JobId=%u non-runnable: %s",
-			     job_ptr->job_id, slurm_strerror(error_code));
-			if (!wiki_sched) {
-				last_job_update = now;
-				job_ptr->job_state = JOB_FAILED;
-				job_ptr->exit_code = 1;
-				job_ptr->state_reason = FAIL_BAD_CONSTRAINTS;
-				xfree(job_ptr->state_desc);
-				job_ptr->start_time = job_ptr->end_time = now;
-				job_completion_logger(job_ptr, false);
-				delete_job_details(job_ptr);
-			}
-		}
-	}
 
 	save_last_part_update = last_part_update;
 	FREE_NULL_BITMAP(avail_node_bitmap);
